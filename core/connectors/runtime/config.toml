# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

[iggy]
address = "localhost:8090"
username = "iggy"
password = "iggy"
# token = "secret" # Personal Access Token (PAT) can be used instead of username and password

[sinks.stdout]
enabled = true
name = "Stdout sink"
path = "target/release/libiggy_connector_stdout_sink"

[[sinks.stdout.streams]]
stream = "example_stream"
topics = ["example_topic"]
schema = "json"
batch_size = 100
poll_interval = "5ms"
consumer_group = "qw_sink_connector"

[sinks.stdout.transforms.add_fields]
enabled = true

[[sinks.stdout.transforms.add_fields.fields]]
key = "message"
value.static = "hello"

[sources.random]
enabled = true
name = "Random source"
path = "target/release/libiggy_connector_random_source"

[[sources.random.streams]]
stream = "example"
topic = "topic1"
schema = "json"
batch_size = 1000
send_interval = "5ms"

[sources.random.config]
interval = "100ms"
# max_count = 1000
messages_range = [10, 50]
payload_size = 200

[sources.random.transforms.add_fields]
enabled = true

[[sources.random.transforms.add_fields.fields]]
key = "test_field"
value.static = "hello!"

[sinks.quickwit]
enabled = true
name = "Quickwit sink 1"
path = "target/release/libiggy_connector_quickwit_sink"

[[sinks.quickwit.streams]]
stream = "qw"
topics = ["records"]
schema = "json"
batch_size = 1000
poll_interval = "5ms"
consumer_group = "qw_sink_connector"

[sinks.quickwit.transforms.add_fields]
enabled = true

[[sinks.quickwit.transforms.add_fields.fields]]
key = "service_name"
value.static = "qw_connector"

[[sinks.quickwit.transforms.add_fields.fields]]
key = "timestamp"
value.computed = "timestamp_millis"

[[sinks.quickwit.transforms.add_fields.fields]]
key = "random_id"
value.computed = "uuid_v7"

[sinks.quickwit.transforms.delete_fields]
enabled = true
fields = ["email", "created_at"]

[sinks.quickwit.config]
url = "http://localhost:7280"
index = """
version: 0.9

index_id: events

doc_mapping:
  mode: strict
  field_mappings:
    - name: timestamp
      type: datetime
      input_formats: [unix_timestamp]
      output_format: unix_timestamp_nanos
      indexed: false
      fast: true
      fast_precision: milliseconds
    - name: service_name
      type: text
      tokenizer: raw
      fast: true
    - name: random_id
      type: text
      tokenizer: raw
      fast: true
    - name: user_id
      type: text
      tokenizer: raw
      fast: true
    - name: user_type
      type: u64
      fast: true
    - name: source
      type: text
      tokenizer: default
    - name: state
      type: text
      tokenizer: default
    - name: message
      type: text
      tokenizer: default

  timestamp_field: timestamp

indexing_settings:
  commit_timeout_secs: 10

retention:
  period: 7 days
  schedule: daily
"""

# Elasticsearch Source配置
[sources.elasticsearch]
enabled = true
name = "Elasticsearch source"
path = "target/release/libiggy_connector_elasticsearch_source"
config_format = "json"

[[sources.elasticsearch.streams]]
stream = "elasticsearch_stream"
topic = "documents"
schema = "json"
batch_length = 100
linger_time = "5ms"

[sources.elasticsearch.config]
url = "http://localhost:9200"
index = "logs-*"
polling_interval = "30s"
batch_size = 100
timestamp_field = "@timestamp"
query = {
"match_all": {}
}
state = {
  enabled = true
  storage_type = "file"
  storage_config = {
    base_path = "./connector_states"
  }
  state_id = "elasticsearch_logs_connector"
  auto_save_interval = "5m"
  tracked_fields = ["last_poll_timestamp", "last_document_id", "total_documents_fetched"]
}

# Elasticsearch Sink配置
[sinks.elasticsearch]
enabled = true
name = "Elasticsearch sink"
path = "target/release/libiggy_connector_elasticsearch_sink"
config_format = "json"

[[sinks.elasticsearch.streams]]
stream = "data_stream"
topics = ["events"]
schema = "json"
batch_length = 50
poll_interval = "5ms"
consumer_group = "elasticsearch_sink_connector"

[sinks.elasticsearch.config]
url = "http://localhost:9200"
index = "iggy-events"
batch_size = 100
timeout_seconds = 30
create_index_if_not_exists = true
index_mapping = {
  "mappings": {
    "properties": {
      "_iggy_offset": { "type": "long" },
      "_iggy_stream": { "type": "keyword" },
      "_iggy_topic": { "type": "keyword" },
      "_iggy_partition": { "type": "integer" },
      "_iggy_timestamp": { "type": "date" }
    }
  }
}

[sinks.elasticsearch.transforms.add_fields]
enabled = true

[[sinks.elasticsearch.transforms.add_fields.fields]]
key = "ingestion_timestamp"
value.computed = "timestamp_millis"